{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3fa7cd6-1121-4088-8b21-5c9417e24842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries; functions to load graps, calculate feedback loops \n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "def load_graph(node_file, edge_file):\n",
    "    \"\"\"\n",
    "    Loads a directed graph from node and edge list CSV files.\n",
    "    :param node_file: Path to the node list CSV file\n",
    "    :param edge_file: Path to the edge list CSV file\n",
    "    :return: A directed graph (DiGraph)\n",
    "    \"\"\"\n",
    "    # Load node list\n",
    "    nodes_df = pd.read_csv(node_file, dtype={\"Node.ID\": str})\n",
    "    \n",
    "    # Load edge list\n",
    "    edges_df = pd.read_csv(edge_file, dtype={\"Source.Node.ID\": str, \"Sink.Node.ID\": str})\n",
    "    \n",
    "    # Create directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for _, row in nodes_df.iterrows():\n",
    "        G.add_node(row[\"Node.ID\"], description=row[\"Node.Description\"])\n",
    "    \n",
    "    # Add edges\n",
    "    for _, row in edges_df.iterrows():\n",
    "        G.add_edge(row[\"Source.Node.ID\"], row[\"Sink.Node.ID\"], weight=row[\"Value\"])\n",
    "    \n",
    "    return G\n",
    "\n",
    "def find_unique_loops(G):\n",
    "    \"\"\"\n",
    "    Finds unique loops (cycles) in a directed graph and determines their polarity.\n",
    "    :param G: A directed graph (DiGraph)\n",
    "    :return: A Pandas DataFrame containing unique loops with polarity (Reinforcing or Balancing)\n",
    "    \"\"\"\n",
    "    cycles = list(nx.simple_cycles(G))  # Get all cycles\n",
    "    unique_cycles = []\n",
    "    seen = set()\n",
    "    \n",
    "    reinforcing_count = 0\n",
    "    balancing_count = 0\n",
    "    loop_data = []\n",
    "\n",
    "    for cycle in cycles:\n",
    "        sorted_cycle = tuple(sorted(cycle))  # Sort cycle for uniqueness\n",
    "        if sorted_cycle not in seen:\n",
    "            seen.add(sorted_cycle)\n",
    "            \n",
    "            # Calculate polarity\n",
    "            polarity = 1\n",
    "            edges = []\n",
    "            for i in range(len(cycle)):\n",
    "                source = cycle[i]\n",
    "                target = cycle[(i + 1) % len(cycle)]\n",
    "                weight = G[source][target]['weight']\n",
    "                polarity *= weight\n",
    "                edges.append(f\"{source} -> {target} ({weight})\")\n",
    "\n",
    "            loop_type = \"Reinforcing\" if polarity == 1 else \"Balancing\"\n",
    "\n",
    "            # Assign loop number\n",
    "            if loop_type == \"Reinforcing\":\n",
    "                reinforcing_count += 1\n",
    "                loop_id = f\"R{reinforcing_count}\"\n",
    "            else:\n",
    "                balancing_count += 1\n",
    "                loop_id = f\"B{balancing_count}\"\n",
    "\n",
    "            loop_data.append([loop_id, \", \".join(edges), loop_type])\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    df_loops = pd.DataFrame(loop_data, columns=[\"Loop #\", \"Edges\", \"Polarity\"])\n",
    "    return df_loops  # Ensure the function returns a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2068dc-1377-439e-9ee8-227186607078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare node lists. Not used in workflow. Differences of variable name, spelling not important.\n",
    "\n",
    "def compare_node_lists(ground_truth_file, generated_file):\n",
    "    \"\"\"\n",
    "    Compares the Generated nodes.csv against Ground Truth nodes.csv using precision, recall, and F1-score.\n",
    "    :param ground_truth_file: Path to the ground truth node list CSV file\n",
    "    :param generated_file: Path to the generated node list CSV file\n",
    "    :return: Precision, Recall, and F1-score\n",
    "    \"\"\"\n",
    "    # Load node lists\n",
    "    ground_truth_df = pd.read_csv(ground_truth_file, dtype={\"Node.ID\": str})\n",
    "    generated_df = pd.read_csv(generated_file, dtype={\"Node.ID\": str})\n",
    "    \n",
    "    # Extract node descriptions and normalize case\n",
    "    ground_truth_nodes = set(ground_truth_df[\"Node.Description\"].str.strip().str.lower().tolist())\n",
    "    generated_nodes = set(generated_df[\"Node.Description\"].str.strip().str.lower().tolist())\n",
    "    \n",
    "    # Compute true positives, false positives, and false negatives\n",
    "    true_positives = len(ground_truth_nodes & generated_nodes)\n",
    "    false_positives = len(generated_nodes - ground_truth_nodes)\n",
    "    false_negatives = len(ground_truth_nodes - generated_nodes)\n",
    "    \n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa68966-45af-4cc2-9a67-f0c7f13dd118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to comapre two Edge lists\n",
    "\n",
    "def compare_edge_lists(ground_truth_file, generated_file):\n",
    "    \"\"\"\n",
    "    Compares the Generated edges.csv against Ground Truth edges.csv using precision, recall, and F1-score.\n",
    "    :param ground_truth_file: Path to the ground truth edge list CSV file\n",
    "    :param generated_file: Path to the generated edge list CSV file\n",
    "    :return: Precision, Recall, and F1-score\n",
    "    \"\"\"\n",
    "    # Load edge lists\n",
    "    ground_truth_df = pd.read_csv(ground_truth_file, dtype={\"Source.Node.ID\": str, \"Sink.Node.ID\": str, \"Value\": int})\n",
    "    generated_df = pd.read_csv(generated_file, dtype={\"Source.Node.ID\": str, \"Sink.Node.ID\": str, \"Value\": int})\n",
    "    \n",
    "    # Extract edge sets as tuples (source, sink, value)\n",
    "    ground_truth_edges = set(tuple(row) for row in ground_truth_df.to_records(index=False))\n",
    "    generated_edges = set(tuple(row) for row in generated_df.to_records(index=False))\n",
    "    \n",
    "    # Compute true positives, false positives, and false negatives\n",
    "    true_positives = len(ground_truth_edges & generated_edges)\n",
    "    false_positives = len(generated_edges - ground_truth_edges)\n",
    "    false_negatives = len(ground_truth_edges - generated_edges)\n",
    "    \n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "423cee68-2ae2-4a5d-83bc-ef4c67e1f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate betweenness centrality\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_betweenness_centrality(node_file, edge_file, weight_penalty_for_negative_link):\n",
    "    \"\"\"\n",
    "    Calculate betweenness centrality for a network with the option to penalize negative links.\n",
    "    \n",
    "    Parameters:\n",
    "    node_file (str): Path to the nodes CSV file\n",
    "    edge_file (str): Path to the edges CSV file\n",
    "    weight_penalty_for_negative_link (float): Weight to replace negative link values\n",
    "    \n",
    "    Returns:\n",
    "    networkx.Graph: The constructed graph with all attributes\n",
    "    dict: Betweenness centrality values for each node\n",
    "    \"\"\"\n",
    "    # Initialize an empty graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Read nodes file\n",
    "    with open(node_file, \"r\") as f:\n",
    "        headers = f.readline().strip().split(',')  # Read the header line\n",
    "        \n",
    "        # Get column indices (handling different file formats)\n",
    "        node_id_idx = headers.index('Node.ID') if 'Node.ID' in headers else 0\n",
    "        node_desc_idx = headers.index('Node.Description') if 'Node.Description' in headers else 1\n",
    "        \n",
    "        for line in f:\n",
    "            data = line.strip().split(\",\")\n",
    "            node_id = data[node_id_idx]\n",
    "            description = data[node_desc_idx] if len(data) > node_desc_idx else \"No Description\"\n",
    "            \n",
    "            # Add node with description attribute\n",
    "            G.add_node(node_id, description=description)\n",
    "    \n",
    "    # Read edges file\n",
    "    with open(edge_file, \"r\") as f:\n",
    "        headers = f.readline().strip().split(',')  # Read the header line\n",
    "        \n",
    "        # Get column indices (handling different file formats)\n",
    "        source_idx = headers.index('Source.Node.ID') if 'Source.Node.ID' in headers else 0\n",
    "        target_idx = headers.index('Sink.Node.ID') if 'Sink.Node.ID' in headers else 1\n",
    "        weight_idx = headers.index('Value') if 'Value' in headers else 2\n",
    "        \n",
    "        for line in f:\n",
    "            data = line.strip().split(\",\")\n",
    "            source = data[source_idx]\n",
    "            target = data[target_idx]\n",
    "            \n",
    "            # Handle weight and apply penalty for negative links\n",
    "            raw_weight = float(data[weight_idx])\n",
    "            if raw_weight < 0:\n",
    "                weight = weight_penalty_for_negative_link\n",
    "            else:\n",
    "                weight = raw_weight\n",
    "            \n",
    "            # Add edge with weight\n",
    "            G.add_edge(source, target, weight=weight)\n",
    "    \n",
    "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "    \n",
    "    # Calculate weighted betweenness centrality\n",
    "    centrality = nx.betweenness_centrality(G, weight=\"weight\")\n",
    "    \n",
    "    # Export results to CSV\n",
    "    with open(\"Betweenness_Centrality.csv\", \"w\") as f:\n",
    "        f.write(\"Node.ID,Node.Description,Between.Centrality\\n\")\n",
    "        for node in G.nodes():\n",
    "            description = G.nodes[node].get('description', 'No Description')\n",
    "            betweenness = centrality.get(node, 0)\n",
    "            f.write(f\"{node},{description},{betweenness}\\n\")\n",
    "    \n",
    "    print(\"Exported Betweenness_Centrality.csv\")\n",
    "    \n",
    "    return G, centrality\n",
    "\n",
    "# Example usage:\n",
    "# G, centrality = calculate_betweenness_centrality(\"nodes.csv\", \"edges.csv\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72149513-b6e2-4238-a144-7a1a8543a243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Edge List Comparison ChatGPT:\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1-score: 1.00\n",
      "\n",
      "Edge List Comparison ClaudeAI:\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1-score: 1.00\n",
      "\n",
      "Edge List Comparison DeepSeek:\n",
      "Precision: 1.00\n",
      "Recall: 0.97\n",
      "F1-score: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Code snippet to run accuracy results. Save the files as follows first then just run this\n",
    "# ground_truth_file = \"edges_ground_truth.csv\"\n",
    "# For the three LLMs, use :-\n",
    "# \"edges_ChatGPT.csv\"\n",
    "# \"edges_claude.csv\"\n",
    "# \"edges_deepseek.csv\"\n",
    "\n",
    "\n",
    "print(\"\\nEdge List Comparison ChatGPT:\")\n",
    "\n",
    "ground_truth_file = \"edges_ground_truth.csv\"\n",
    "generated_file = \"edges_ChatGPT.csv\"\n",
    "precision, recall, f1 = compare_edge_lists(ground_truth_file, generated_file)\n",
    "    \n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "print(\"\\nEdge List Comparison ClaudeAI:\")\n",
    "\n",
    "ground_truth_file = \"edges_ground_truth.csv\"\n",
    "generated_file = \"edges_claude.csv\"\n",
    "precision, recall, f1 = compare_edge_lists(ground_truth_file, generated_file)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "print(\"\\nEdge List Comparison DeepSeek:\")\n",
    "\n",
    "ground_truth_file = \"edges_ground_truth.csv\"\n",
    "generated_file = \"edges_deepseek.csv\"\n",
    "precision, recall, f1 = compare_edge_lists(ground_truth_file, generated_file)\n",
    "    \n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84baba0e-5bf6-42be-a0c0-b543782b0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code snippet to enumerate all the loops for the three LLMs\n",
    "# node file = \"nodes_ground_truth.csv\"\n",
    "# For the three LLMs, use :-\n",
    "# \"edges_ChatGPT.csv\"\n",
    "# \"edges_claude.csv\"\n",
    "# \"edges_deepseek.csv\"\n",
    "\n",
    "node_file = \"Nodes_ground_truth.csv\"\n",
    "edge_file = \"Edges_chatgpt.csv\"\n",
    "    \n",
    "G = load_graph(node_file, edge_file)\n",
    "loop_df = find_unique_loops(G)  \n",
    "loop_df.to_csv(\"loops_ChatGPT.csv\", index=False)\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "node_file = \"Nodes_ground_truth.csv\"\n",
    "edge_file = \"Edges_claude.csv\"\n",
    "    \n",
    "G = load_graph(node_file, edge_file)\n",
    "loop_df.to_csv(\"loops_Claude.csv\", index=False)\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "node_file = \"Nodes_ground_truth.csv\"\n",
    "edge_file = \"Edges_deepseek.csv\"\n",
    "    \n",
    "G = load_graph(node_file, edge_file)\n",
    "loop_df = find_unique_loops(G)  \n",
    "loop_df.to_csv(\"loops_deepseek.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67a0a440-ee3d-44a2-bd39-871a36a41704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 17\n",
      "Number of edges: 29\n",
      "Exported Betweenness_Centrality.csv\n"
     ]
    }
   ],
   "source": [
    "# Code to call network analysis/get betweenness\n",
    "G, centrality = calculate_betweenness_centrality(\"nodes_ground_truth.csv\", \"edges_ground_truth.csv\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d5db2-2d52-4b2f-a0f7-4d126960665c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
