{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b5ee5c-5274-40b2-8011-adf2176f9a6c",
   "metadata": {},
   "source": [
    "## Setting Up Chain-of-Thought (COT) prompting for 3 LLMs (GPT-4o, Claude-4, Deepseek R1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995e9a4-6a25-41b5-87da-b2a3f3300fac",
   "metadata": {},
   "source": [
    "*Packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d0783df616c30bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:27:30.744083Z",
     "start_time": "2025-06-16T08:27:30.740959Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701e839-bbde-4dff-820d-4c07b3c0ea78",
   "metadata": {},
   "source": [
    "*API call functions for OpenAI GPT, Anthropic Claude, and Deepseek*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43f53f8256e686e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:33:16.431783Z",
     "start_time": "2025-06-16T08:33:16.424304Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def deepseek_chat(messages, model=\"deepseek-chat\") -> str:\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        base_url=\"https://api.deepseek.com\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def _chat(model: str, messages: List[Dict], max_tokens: int = 1024) -> str:\n",
    "    if model.startswith(\"gpt\"):\n",
    "        try:\n",
    "            client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except AttributeError:\n",
    "            openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    elif model.startswith(\"claude\"):\n",
    "        client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "        prompt = \"\\n\\n\".join(f\"{m['role'].capitalize()}: {m['content']}\" for m in messages) + \"\\n\\nAssistant:\"\n",
    "        response = client.messages.create(\n",
    "            model=model,\n",
    "            system=\"You are a helpful assistant skilled in causal reasoning and CLDs.\",\n",
    "            max_tokens=max_tokens,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "    elif model.startswith(\"deepseek\"):\n",
    "        return deepseek_chat(messages, model=model)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e42e23-afe4-4cbc-a129-a06526931878",
   "metadata": {},
   "source": [
    "*Function to chain prompt sequence to realise the chain-of-thought prompting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab6e992e2bcee2e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:32:09.328289Z",
     "start_time": "2025-06-16T08:32:09.323687Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_llm_chain(model: str, prompt_sequence: List[str]) -> List[Dict]:\n",
    "    messages = []\n",
    "    history = []\n",
    "\n",
    "    for idx, prompt in enumerate(prompt_sequence, start=1):\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        reply = _chat(model, messages)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        history.append({\n",
    "            \"stage\": idx,\n",
    "            \"prompt\": prompt,\n",
    "            \"reply\": reply\n",
    "        })\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e60e3-a0dc-4677-97bc-1a0a1f1f3cd7",
   "metadata": {},
   "source": [
    "*Function to run all (or selected models)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edb6c09429385054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:32:09.626503Z",
     "start_time": "2025-06-16T08:32:09.621974Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_all_models(models: List[str], prompt_sequence: List[str]) -> Dict[str, List[Dict]]:\n",
    "    all_results = {}\n",
    "    for model in models:\n",
    "        print(f\"ðŸš€ Running chain for {model}...\")\n",
    "        try:\n",
    "            result = run_llm_chain(model, prompt_sequence)\n",
    "            all_results[model] = result\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error for {model}: {e}\")\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee5be8-ffe1-4a7e-ae7a-1df912c4045b",
   "metadata": {},
   "source": [
    "### (I) Testing function with 2 prompt toy example"
   ]
  },
  {
   "cell_type": "code",
   "id": "f7b17974-50e5-4094-81f2-07d267eabdb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T09:03:55.462620Z",
     "start_time": "2025-06-16T09:03:55.459144Z"
    }
   },
   "source": [
    "# toy sequence of 2 prompts\n",
    "PROMPT_SEQUENCE = [\n",
    "    \"What is your understanding of the issue with causal inference in traditional statistics?\",\n",
    "    \"Can you explain with example how to represent causal inference in directed acyclic graph?\"\n",
    "]   "
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5deebb1829f9727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:28:02.848223Z",
     "start_time": "2025-06-16T08:27:42.223670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Running chain for gpt-4o...\n"
     ]
    }
   ],
   "source": [
    "models_to_test = [\"gpt-4o\"]\n",
    "results = run_all_models(models_to_test, PROMPT_SEQUENCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39460e8481226f3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:28:05.217513Z",
     "start_time": "2025-06-16T08:28:05.214445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpt-4o': [{'stage': 1, 'prompt': 'What is your understanding of the issue with causal inference in traditional statistics?', 'reply': 'Causal inference is a major challenge in traditional statistics due to its focus on correlation rather than causation. In traditional statistical analysis, we primarily deal with associations or correlations between variables, which do not necessarily imply that one variable causes the change in another. Several key issues contribute to the difficulty of causal inference:\\n\\n1. **Confounding Variables**: These are variables that influence both the independent and dependent variables, creating a spurious association. If not controlled for, confounders can lead to incorrect conclusions regarding causal relationships.\\n\\n2. **Reverse Causation**: This occurs when the direction of cause and effect is opposite to what is assumed. Traditional statistical methods often have difficulty in distinguishing whether X causes Y or Y causes X.\\n\\n3. **Simpsonâ€™s Paradox**: This statistical phenomenon occurs when a trend apparent in several different groups of data disappears or reverses when the groups are combined. It highlights the importance of considering underlying data structure and potential lurking variables.\\n\\n4. **Lack of Experimental Control**: In observational studies, researchers cannot control for all variables, making it difficult to rule out other explanations for an observed effect.\\n\\n5. **Over-reliance on P-Values**: P-values indicate the probability of observing the data assuming the null hypothesis is true, not the probability that the null or alternative hypothesis is true. This can lead to misinterpretation in the context of causation, as statistical significance does not imply a causal relationship.\\n\\nTo address these issues, researchers use causal inference methods and frameworks such as randomized controlled trials (RCTs), which are considered the gold standard for establishing causality, as well as observational causal inference techniques like instrumental variables, difference-in-differences, and propensity score matching. Additionally, the development and application of causal diagrams, like directed acyclic graphs (DAGs), help clarify and test causal assumptions.\\n\\nOverall, causal inference requires careful study design, rigorous testing of assumptions, and often sophisticated methodologies beyond the scope of traditional statistical approaches.'}, {'stage': 2, 'prompt': 'Can you explain with example how to represent causal inference in directed acyclic graph?', 'reply': 'Directed Acyclic Graphs (DAGs) are a useful tool for representing and analyzing causal relationships between variables. They consist of nodes representing variables and directed edges (arrows) indicating causal influence from one variable to another. The \"acyclic\" part means there are no feedback loops; you cannot start at one variable and follow a sequence of directed edges that leads back to the same variable.\\n\\nHere\\'s an example to illustrate how to use DAGs for causal inference:\\n\\n### Scenario\\nImagine you\\'re studying the relationship between exercise (X), diet (Z), and weight loss (Y). You want to understand how exercise causally affects weight loss while considering diet as a potential confounder.\\n\\n### DAG Representation\\n\\n1. **Nodes**: Represent the variables in your study.\\n   - X = Exercise\\n   - Z = Diet\\n   - Y = Weight Loss\\n\\n2. **Edges**: Represent causal relationships between these variables.\\n   - X â†’ Y: Exercise affects weight loss.\\n   - Z â†’ Y: Diet affects weight loss.\\n   - Z â†’ X: Diet influences the amount of exercise.\\n\\n### DAG Diagram\\nHere\\'s how the DAG would look based on the assumed causal relationships:\\n\\n```\\nZ â†’ X â†’ Y\\n  â†˜\\n   â†’ Y\\n```\\n\\n### Interpretation\\n- **X â†’ Y**: Indicates a hypothesized causal effect of exercise on weight loss.\\n- **Z â†’ X and Z â†’ Y**: These edges suggest that diet is a confounder. It affects both exercise and weight loss, potentially biasing the estimate of the causal effect of exercise on weight loss if not properly controlled.\\n\\n### Causal Inference Insights\\n- **Confounding Control**: In this setting, Z (diet) is a confounder because it influences both the treatment (exercise) and the outcome (weight loss). By adjusting for Z, either through statistical methods or study design (e.g., randomization), you can more accurately estimate the causal effect of exercise on weight loss.\\n- **Path Blocking**: To infer the direct causal effect of exercise on weight loss, you need to block the backdoor path `X â† Z â†’ Y` by controlling for Z.\\n\\n### Conclusion\\nDAGs help clarify complex causal relationships by visually mapping out the assumptions and potential confounders. In causal inference, they guide the selection of appropriate statistical methods to estimate causal effects, ensuring that analyses are aligned with the underlying causal framework.'}]}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76f2281a3dfc1ffd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:29:10.495021Z",
     "start_time": "2025-06-16T08:28:31.719022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Running chain for claude-opus-4-20250514...\n"
     ]
    }
   ],
   "source": [
    "models_to_test = [\"claude-opus-4-20250514\"]\n",
    "results = run_all_models(models_to_test, PROMPT_SEQUENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da86dd671f46627d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:29:22.559192Z",
     "start_time": "2025-06-16T08:29:22.554185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claude-opus-4-20250514': [{'stage': 1,\n",
       "   'prompt': 'What is your understanding of the issue with causal inference in traditional statistics?',\n",
       "   'reply': 'Traditional statistics faces several fundamental challenges with causal inference:\\n\\n## The Core Problem: Correlation â‰  Causation\\n\\nTraditional statistical methods excel at identifying associations and correlations but cannot, by themselves, determine whether these relationships are causal. A significant correlation between two variables could arise from:\\n- A causing B\\n- B causing A  \\n- A third variable C causing both A and B (confounding)\\n- Pure coincidence\\n\\n## Key Limitations\\n\\n**1. Observational Data Ambiguity**\\nMost real-world data is observational rather than experimental. Without random assignment, we can\\'t isolate causal effects from confounding factors. Traditional regression models can control for observed confounders, but unobserved confounders remain problematic.\\n\\n**2. Lack of Causal Framework**\\nTraditional statistics lacks formal notation and frameworks for expressing causal assumptions. Concepts like \"controlling for\" variables are often used without clear causal justification.\\n\\n**3. Simpson\\'s Paradox**\\nStatistical relationships can reverse when data is aggregated or disaggregated, making it unclear which level of analysis reveals the \"true\" causal relationship.\\n\\n**4. Selection Bias and Colliders**\\nConditioning on certain variables (colliders) can actually introduce bias rather than remove it - something traditional statistical training often overlooks.\\n\\n## Modern Solutions\\n\\nCausal inference has evolved beyond traditional statistics through:\\n- Directed Acyclic Graphs (DAGs) to explicitly model causal assumptions\\n- Pearl\\'s causal hierarchy (association, intervention, counterfactuals)\\n- Potential outcomes framework\\n- Natural experiments and instrumental variables\\n- Causal discovery algorithms\\n\\nThese approaches provide formal frameworks for moving from correlation to causation when certain assumptions are met.'},\n",
       "  {'stage': 2,\n",
       "   'prompt': 'Can you explain with example how to represent causal inference in directed acyclic graph?',\n",
       "   'reply': 'I\\'ll explain how DAGs represent causal inference with a concrete example.\\n\\n## Example: Does Education Cause Higher Income?\\n\\nLet\\'s explore whether getting a college education causes higher income using a DAG.\\n\\n### Basic DAG Structure\\n\\nIn a DAG:\\n- **Nodes (circles)** = Variables\\n- **Arrows** = Direct causal relationships\\n- **Arrow direction** = Cause â†’ Effect\\n\\n### Simple (Naive) Model\\n\\n```\\nEducation â†’ Income\\n```\\n\\nThis suggests education directly causes income. But is it that simple?\\n\\n### More Realistic Model\\n\\nLet\\'s add confounding variables:\\n\\n```\\n    Family Wealth\\n       â†™     â†˜\\nEducation â†’ Income\\n       â†˜     â†—\\n      Ability\\n```\\n\\nNow we see:\\n- **Family Wealth** affects both Education (can afford college) and Income (connections, inheritance)\\n- **Ability** affects both Education (admission, completion) and Income (job performance)\\n- These are **confounders** creating non-causal associations\\n\\n### Adding More Complexity\\n\\n```\\n    Family Wealth\\n       â†™     â†˜\\nEducation â†’ Income â† Location\\n   â†“           â†‘\\nJob Type â† Industry Demand\\n```\\n\\n## Key Causal Concepts in DAGs\\n\\n**1. Confounders (Common Causes)**\\n- Family Wealth and Ability are confounders\\n- Must control for these to estimate causal effect\\n\\n**2. Mediators (Causal Pathways)**\\n- Job Type mediates between Education and Income\\n- Don\\'t control for mediators if you want total effect\\n\\n**3. Colliders (Common Effects)**\\n- If we added \"Loan Default\" caused by both low Income and high Education\\n- Should NOT control for colliders\\n\\n## Using DAGs for Analysis\\n\\n### Identifying What to Control For\\n\\nTo find Education\\'s causal effect on Income:\\n1. List all paths from Education to Income\\n2. Block non-causal \"backdoor\" paths by controlling for confounders\\n3. Keep causal paths open\\n\\n**Paths in our example:**\\n- Education â†’ Income (causal âœ“)\\n- Education â† Family Wealth â†’ Income (backdoor - must block)\\n- Education â† Ability â†’ Income (backdoor - must block)\\n- Education â†’ Job Type â†’ Income (causal âœ“)\\n\\n**Conclusion:** Control for Family Wealth and Ability, but not Job Type\\n\\n### The Power of DAGs\\n\\nDAGs make causal assumptions explicit and testable:\\n- We can derive testable implications\\n- We know exactly what to control for\\n- We avoid controlling for the wrong variables\\n- We can identify when causal inference is impossible\\n\\nThis systematic approach transforms vague notions of \"correlation vs causation\" into precise, actionable analysis strategies.'}]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6335b5838ff904dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:38:32.328601Z",
     "start_time": "2025-06-16T08:37:27.433033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Running chain for deepseek-chat...\n"
     ]
    }
   ],
   "source": [
    "models_to_test = [\"deepseek-chat\"] # note that this is for DeepSeek V1\n",
    "results = run_all_models(models_to_test, PROMPT_SEQUENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f81a8e770d75287d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:38:39.339399Z",
     "start_time": "2025-06-16T08:38:39.334498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deepseek-chat': [{'stage': 1,\n",
       "   'prompt': 'What is your understanding of the issue with causal inference in traditional statistics?',\n",
       "   'reply': 'Causal inference is a fundamental challenge in traditional statistics because statistical methods alone cannot establish causation without strong assumptions or experimental control. Hereâ€™s a breakdown of the key issues:\\n\\n### 1. **Correlation â‰  Causation**\\n   - Traditional statistics often focuses on identifying associations (e.g., regression, correlation), but association does not imply causation. Confounding variables may explain observed relationships.\\n   - Example: Ice cream sales and drowning incidents are correlated (both rise in summer), but neither causes the other.\\n\\n### 2. **Lack of Counterfactuals**\\n   - Causal questions require comparing what happened to what *would have happened* under alternative conditions (counterfactuals). Traditional observational data lacks this because we cannot observe the same unit under both treatment and control simultaneously.\\n   - Statistics alone cannot measure counterfactuals without additional assumptions (e.g., ignorability, exchangeability).\\n\\n### 3. **Confounding Bias**\\n   - Unmeasured or uncontrolled confounders can distort causal estimates. Traditional regression adjusts for observed confounders but fails if key variables are missing or unmeasured.\\n   - Example: Studying the effect of education on income without accounting for innate ability or socioeconomic background.\\n\\n### 4. **Selection Bias**\\n   - Non-random assignment to treatment groups (e.g., in observational studies) leads to biased estimates. Traditional methods like ANOVA or regression may not correct for this unless properly adjusted (e.g., propensity scoring).\\n\\n### 5. **Overreliance on Linear Models**\\n   - Many traditional methods assume linearity or parametric forms, which may not capture complex causal relationships (e.g., interactions, nonlinear effects).\\n\\n### 6. **No Formal Causal Language**\\n   - Traditional statistics often lacks explicit frameworks to define and test causal hypotheses. Tools like Directed Acyclic Graphs (DAGs) or potential outcomes models are newer additions to address this.\\n\\n### 7. **Experimental vs. Observational Data**\\n   - Randomized Controlled Trials (RCTs) are the gold standard for causal inference because they eliminate confounding via randomization. Traditional observational statistics struggles to replicate this rigor without strong assumptions.\\n\\n### Modern Solutions:\\nTo address these issues, modern causal inference extends traditional statistics with:\\n   - **Potential Outcomes Framework** (Rubin Causal Model): Formalizes counterfactuals.\\n   - **Structural Causal Models** (Pearlâ€™s DAGs): Maps assumptions about causal relationships.\\n   - **Instrumental Variables (IV), Difference-in-Differences (DiD), Regression Discontinuity (RD)**: Quasi-experimental methods to approximate causality.\\n   - **Causal Machine Learning**: Combines traditional methods with ML for flexibility.\\n\\n### Key Takeaway:\\nTraditional statistics is powerful for description and prediction but limited for causal claims without careful design or assumptions. Causal inference requires explicit modeling of mechanisms, counterfactuals, and confoundingâ€”areas where traditional methods often fall short without augmentation.'},\n",
       "  {'stage': 2,\n",
       "   'prompt': 'Can you explain with example how to represent causal inference in directed acyclic graph?',\n",
       "   'reply': 'Certainly! **Directed Acyclic Graphs (DAGs)** are a powerful tool for representing causal relationships visually. They help clarify assumptions, identify confounders, and determine appropriate statistical adjustments. Hereâ€™s a step-by-step explanation with an example:\\n\\n---\\n\\n### **Key Components of a DAG**\\n1. **Nodes (Variables)**: Represent variables (e.g., treatment, outcome, confounders).\\n2. **Directed Edges (Arrows)**: Indicate causal relationships (e.g., \\\\( A \\\\rightarrow B \\\\) means \\\\( A \\\\) causes \\\\( B \\\\)).\\n3. **Acyclic**: No loops (no variable can cause itself, directly or indirectly).\\n\\n---\\n\\n### **Example: Does Smoking Cause Lung Cancer?**\\nSuppose we want to model the causal effect of smoking (\\\\( S \\\\)) on lung cancer (\\\\( L \\\\)). A simple DAG might include:\\n- **Smoking (\\\\( S \\\\))**: Treatment/exposure.\\n- **Lung Cancer (\\\\( L \\\\))**: Outcome.\\n- **Confounder: Asbestos Exposure (\\\\( A \\\\))**: A variable that affects both smoking and lung cancer.\\n\\n#### **DAG Representation:**\\n```\\n   A\\n  / \\\\\\n S â†’ L\\n```\\n- \\\\( A \\\\rightarrow S \\\\): Asbestos exposure increases likelihood of smoking (e.g., due to occupational stress).\\n- \\\\( A \\\\rightarrow L \\\\): Asbestos exposure directly causes lung cancer.\\n- \\\\( S \\\\rightarrow L \\\\): Smoking causes lung cancer (the causal effect we want to estimate).\\n\\n---\\n\\n### **Interpreting the DAG**\\n1. **Confounding Path**:  \\n   The backdoor path \\\\( S \\\\leftarrow A \\\\rightarrow L \\\\) is non-causal and must be blocked to isolate the effect of \\\\( S \\\\) on \\\\( L \\\\).  \\n   - **Solution**: Condition on \\\\( A \\\\) (e.g., stratify or adjust for asbestos exposure in regression).\\n\\n2. **Collider Variables (Optional Extension)**:  \\n   If we added a variable affected by both \\\\( S \\\\) and \\\\( L \\\\) (e.g., \"Coughing\" \\\\( C \\\\)), it would be a collider (\\\\( S \\\\rightarrow C \\\\leftarrow L \\\\)). Conditioning on colliders can introduce bias (e.g., if we adjust for coughing, we might distort the \\\\( S \\\\)-\\\\( L \\\\) relationship).\\n\\n---\\n\\n### **How to Use the DAG for Analysis**\\n1. **Identify Confounders**: Variables with arrows into both treatment and outcome (e.g., \\\\( A \\\\)).  \\n2. **Block Backdoor Paths**: Adjust for confounders to isolate the causal effect.  \\n   - In this case, controlling for \\\\( A \\\\) gives an unbiased estimate of \\\\( S \\\\rightarrow L \\\\).  \\n3. **Avoid Conditioning on Colliders**: Unless intentional (e.g., for selection bias analysis).\\n\\n---\\n\\n### **Real-World Application**\\nIf you ran a regression:  \\n\\\\( L = \\\\beta S + \\\\gamma A + \\\\epsilon \\\\),  \\nthe coefficient \\\\( \\\\beta \\\\) estimates the causal effect of smoking *after* accounting for asbestos exposure. Without adjusting for \\\\( A \\\\), the estimate would be biased.\\n\\n---\\n\\n### **Why DAGs Matter**\\n- **Clarity**: Visualizes assumptions (e.g., \"Does asbestos cause smoking?\").  \\n- **Bias Detection**: Reveals sources of confounding or selection bias.  \\n- **Guidance**: Determines which variables to adjust for (or ignore) in statistical models.\\n\\nFor deeper study, Judea Pearlâ€™s **\"Book of Why\"** or **\"Causal Inference in Statistics: A Primer\"** are excellent resources. Would you like another example with mediation or instrumental variables?'}]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17032d-9648-4e55-b8ac-146102b37cca",
   "metadata": {},
   "source": [
    "### (II) Testing the result based on the 8 prompts for the 4 stage process to construct CLDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "172281408b357cec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:41:09.107610Z",
     "start_time": "2025-06-16T08:41:09.099457Z"
    }
   },
   "outputs": [],
   "source": [
    "from prompts.system_messages import PROMPT_SEQUENCE"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note that \"deepseek-reasoner\" is for DeepSeek R1 (the reasoning model) and be warned of exceptionally long run-time (>10 min)*",
   "id": "966d0fa5f81656a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de814fa3a099e9d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-16T08:41:11.260286Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Running chain for gpt-4o...\n",
      "ðŸš€ Running chain for claude-opus-4-20250514...\n",
      "ðŸš€ Running chain for deepseek-reasoner...\n"
     ]
    }
   ],
   "source": [
    "models_to_test = [\"gpt-4o\", \"claude-opus-4-20250514\", \"deepseek-reasoner\"]\n",
    "results = run_all_models(models_to_test, PROMPT_SEQUENCE)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Export results as individual JSON*",
   "id": "78de4c2ab562db75"
  },
  {
   "cell_type": "code",
   "id": "2734c8abf5c21c0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T09:10:53.623463Z",
     "start_time": "2025-06-16T09:10:53.615892Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Ensure the subfolder exists\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# Export each model's history into the output folder\n",
    "for model_name, history in results.items():\n",
    "    with open(f\"output/{model_name}_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, ensure_ascii=False, indent=2)\n"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "2ad3f25d-b61a-4bbd-9433-db0a5f18ae39",
   "metadata": {},
   "source": "*Export results as individual csv*"
  },
  {
   "cell_type": "code",
   "id": "353b547b523fe2d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T09:11:13.843223Z",
     "start_time": "2025-06-16T09:11:13.833730Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Export each model's results as a CSV file into output/\n",
    "for model_name, history in results.items():\n",
    "    df = pd.DataFrame(history)\n",
    "    df.to_csv(f\"output/{model_name}_results.csv\", index=False, encoding=\"utf-8\")\n"
   ],
   "outputs": [],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
